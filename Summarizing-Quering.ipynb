{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Packages Needed to be Installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2 requests\n",
    "!pip install pdfminer.six\n",
    "!pip -q install langchain openai tiktoken chromadb\n",
    "!pip install chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import requests\n",
    "import os\n",
    "import PyPDF2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Used to Download the PDF from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url, save_folder, idx):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #Checking if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        pdf_filename = f\"Paper_{idx + 1}\"\n",
    "        #Ensuring the filename has \".pdf\" extension\n",
    "        if not pdf_filename.lower().endswith(\".pdf\"):\n",
    "            pdf_filename = f\"{pdf_filename}.pdf\"\n",
    "        pdf_path = os.path.join(save_folder, pdf_filename)\n",
    "        with open(pdf_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(f\"PDF downloaded and saved as {pdf_filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_urls = [\n",
    "    'https://dl.acm.org/doi/pdf/10.1145/3397271.3401075',\n",
    "    'https://arxiv.org/pdf/2104.07186.pdf',\n",
    "    'https://arxiv.org/pdf/2106.14807.pdf\n",
    "    'https://arxiv.org/pdf/2301.03266.pdf',\n",
    "    'https://arxiv.org/pdf/2303.07678.pdf',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing The Text from URL Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dc\\anaconda\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dc\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_url(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract text from the parsed content\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "    else:\n",
    "        print(f\"Failed to fetch content from {url}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Output directory to store the file as pdf from the given URL\n",
    "\n",
    "output_folder = 'pdfs'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved as Paper_1.pdf\n",
      "PDF downloaded and saved as Paper_2.pdf\n",
      "PDF downloaded and saved as Paper_3.pdf\n",
      "PDF downloaded and saved as Paper_4.pdf\n",
      "PDF downloaded and saved as Paper_5.pdf\n"
     ]
    }
   ],
   "source": [
    "# Downloading each File from URL as PDF and saving it in in output folder \n",
    "for idx, pdf_url in enumerate(pdf_urls):\n",
    "    download_pdf(pdf_url, output_folder, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing Function is used to Extract the text from the Downloaded PDF file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf_to_text(pdf_file):\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsed Content from the PDF file are stored in the form of text file in Another Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_files(parsed_texts, output_folder, pdf_filename):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for idx, text in enumerate(parsed_texts):\n",
    "        text_filename = f\"{pdf_filename}.txt\"\n",
    "        text_filepath = os.path.join(output_folder, text_filename)\n",
    "        with open(text_filepath, \"w\", encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "    print(f\"Parsed content from '{pdf_filename}' saved in '{output_folder}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdfs_and_save_as_text(input_folder, output_folder):\n",
    "    pdf_files = glob.glob(os.path.join(input_folder, \"*.pdf\"))\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_filename = os.path.basename(pdf_file).replace(\".pdf\", \"\")\n",
    "        parsed_text = parse_pdf_to_text(pdf_file)\n",
    "        create_text_files([parsed_text], output_folder, pdf_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed content from 'Paper_1' saved in './parsed_file/'.\n",
      "Parsed content from 'Paper_2' saved in './parsed_file/'.\n",
      "Parsed content from 'Paper_3' saved in './parsed_file/'.\n",
      "Parsed content from 'Paper_4' saved in './parsed_file/'.\n",
      "Parsed content from 'Paper_5' saved in './parsed_file/'.\n"
     ]
    }
   ],
   "source": [
    "parse_pdfs_and_save_as_text(\"./pdfs/\",output_folder=\"./parsed_file/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize from the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\dc\\anaconda\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\dc\\anaconda\\lib\\site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from gensim) (2.0.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\dc\\anaconda\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\dc\\anaconda\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.2.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2022.7)\n",
      "Requirement already satisfied: simpful in c:\\users\\dc\\anaconda\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.11.0)\n",
      "Requirement already satisfied: fst-pso in c:\\users\\dc\\anaconda\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dc\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n",
      "Requirement already satisfied: miniful in c:\\users\\dc\\anaconda\\lib\\site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "\n",
    "def text_summarization(text, ratio=0.2):\n",
    "    with open('summaries.txt', 'w', encoding='utf-8') as output_file:\n",
    "    # Use the TextRank algorithm to summarize the text\n",
    "        summarized_text = summarize(text, ratio=ratio)\n",
    "        output_file.write(summarized_text + \"\\n\\n\")\n",
    "        return summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the LangChain for summarizing the content from the PDF files using Openai API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing Multiple PDFs and the summarized File can be Downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import OpenAI, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.2)\n",
    "\n",
    "def summarize_pdfs_from_folder(pdfs_folder):\n",
    "    summaries = []\n",
    "    with open('summaries.txt', 'w', encoding='utf-8') as output_file:\n",
    "        for pdf_file in glob.glob(pdfs_folder + \"/*.pdf\"):\n",
    "            loader = PyPDFLoader(pdf_file)\n",
    "            docs = loader.load_and_split()\n",
    "            chain = load_summarize_chain(llm, chain_type = \"map_reduce\")\n",
    "            summary = chain.run(docs)\n",
    "            output_file.write(f\"Summary for: {pdf_file}\\n\")\n",
    "            output_file.write(summary + \"\\n\\n\")\n",
    "            summaries.append(summary)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for:  ./pdfs\\Paper_1.pdf\n",
      " This paper presents ColBERT, a novel ranking model that adapts deep language models such as BERT for efficient retrieval. It uses a late interaction architecture to independently encode the query and the document, and employs a cheap yet powerful interaction step to model their fine-grained similarity. Results show that ColBERT is competitive with existing BERT-based models in terms of effectiveness, while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query. It is evaluated on MS MARCO and TREC CAR datasets, and its reference implementation is released as open source.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for:  ./pdfs\\Paper_2.pdf\n",
      "\n",
      "\n",
      "This paper introduces a novel retrieval system, COIL, which combines the efficiency of exact match and the representation power of deep language models. It uses an inverted list indexing to store document vectors and vector similarity to compute matching scores. Experiments show that COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency. It is also able to mitigate vocabulary mismatch with a high-level CLS vector matching and provides a step towards building a next-generation index that stores semantics.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for:  ./pdfs\\Paper_3.pdf\n",
      " This paper presents a novel conceptual framework for understanding recent developments in information retrieval, which is organized along two dimensions: sparse vs. dense representations and unsupervised vs. learned representations. It introduces a new technique called \"uniCOIL\" which achieves the current state-of-the-art in sparse retrieval on the MS MARCO passage ranking dataset. It also discusses various approaches to passage retrieval for open-domain question answering, including Learning Passage Impacts for Inverted Indexes, From doc2query to docTTTTTquery, RocketQA, LDA-based Document Models for Ad-hoc Retrieval, Global Term Weights for Document Retrieval Learned from TREC Data, Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval, Efficient Passage Retrieval with Hashing for Open-Domain Question Answering, and Anserini.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for:  ./pdfs\\Paper_4.pdf\n",
      "\n",
      "\n",
      "Doc2Query is a technique used to improve the effectiveness of search engines by expanding documents before indexing. Gospodinov et al. explored the effects of relevance filtering on the retrieval effectiveness of Doc2Query models and found that it can improve retrieval effectiveness by up to 16%, reduce query execution time by 23%, and reduce index size by 33%. This paper also presents 25 research papers that explore various aspects of information retrieval, such as deeper text understanding, context-aware document term weighting, and multi-step retriever-reader interaction.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-l6PgmUGoM4u3TCQP6hDBScVG on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for:  ./pdfs\\Paper_5.pdf\n",
      " This paper presents a query expansion approach, called query2doc, which uses language models to generate pseudo-documents to improve sparse and dense retrieval systems. Results show that query2doc boosts the performance of BM25 by 3-15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. It also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results. Additionally, the paper discusses four different methods for dense passage retrieval and provides details on the hyperparameters used for training the dense retrievers.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summaries = summarize_pdfs_from_folder(\"./pdfs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary for: Paper_1.pdf\n",
    " This paper presents ColBERT, a novel ranking model that adapts deep language models such as BERT for efficient retrieval. \n",
    "It uses a late interaction architecture to independently encode the query and the document, and employs a cheap yet powerful interaction step to model their \n",
    "fine-grained similarity. Results show that ColBERT is competitive with existing BERT-based models in terms of effectiveness, while executing two orders-of-magnitude faster \n",
    "and requiring up to four orders-of-magnitude fewer FLOPs per query. It is evaluated on MS MARCO and TREC CAR datasets, and its reference implementation is released as open \n",
    "source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting the Parsed Content into the Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Processing Multiple Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('./parsed_file/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the Text\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='including DRMM [ 7], KNRM [ 4,36], and Duet [ 20,22]. In contrast\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than the\\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\\nand/or a fee. Request permissions from permissions@acm.org.\\nSIGIR ’20, July 25–30, 2020, Virtual Event, China\\n©2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 978-1-4503-8016-4/20/07. . . $15.00\\nhttps://doi.org/10.1145/3397271.3401075\\n0.15 0.20 0.25 0.30 0.35 0.40\\nMRR@10101102103104105Query Latency (ms)\\nBM25doc2queryKNRMDuet\\nDeepCTfT+ConvKNRM', metadata={'source': 'parsed_file\\\\Paper_1.txt'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating The Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embbed and store the text\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "\n",
    "persist_directory = 'db'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the persisted database from disk and use it as normal\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"what is ColBert?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a Chain such that the parsed content can be retrieved from the DB using RAG method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Chain to answer the question\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(),\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever = retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ColBert is a novel ranking model that employs contextualized late interaction over deep language models (in particular, BERT) for efficient retrieval. It allows for scaling to end-to-end neural retrieval directly from a large document collection, which can improve recall over existing models.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is ColBert?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the model gpt-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Setting up turbo LLM\n",
    "turbo_llm = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       retriever=retriever,\n",
    "                                       return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process_llm_response Function is used to retrieve the answers to the respective query from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information retrieval (IR) techniques are used to locate relevant documents from a large corpus based on a user's query. There are two main paradigms for IR: lexical-based sparse retrieval and embedding-based dense retrieval.\n",
      "\n",
      "1. Lexical-based sparse retrieval: This approach uses algorithms like BM25 (Best Match 25) to match the query terms with the terms in the documents. It relies on the frequency and distribution of the query terms in the documents to determine relevance. This approach is widely used and performs well on out-of-domain datasets.\n",
      "\n",
      "2. Embedding-based dense retrieval: This approach utilizes neural network models to represent the queries and documents as dense vectors in a high-dimensional space. The similarity between the query and documents is measured based on the proximity of their vector representations. Dense retrievers, such as BERT (Bidirectional Encoder Representations from Transformers), have shown improved performance when large amounts of labeled data are available.\n",
      "\n",
      "Query expansion is another technique used in IR. It involves rewriting the query based on pseudo-relevance feedback or external knowledge sources to improve retrieval accuracy. This technique has been used for a long time and has proven to be effective in enhancing retrieval results.\n",
      "\n",
      "Overall, IR techniques aim to improve the precision and relevance of document retrieval from a large corpus, and researchers continue to explore new methods and approaches to enhance the performance of these techniques.\n",
      "\n",
      "\n",
      "Sources:\n",
      "parsed_file\\Paper_5.txt\n",
      "parsed_file\\Paper_1.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain Information Retrival Techniques\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT' are Omar Khattab and Matei Zaharia.\n",
      "\n",
      "\n",
      "Sources:\n",
      "parsed_file\\Paper_1.txt\n",
      "parsed_file\\Paper_3.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"who is the Author of 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT'?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In paper_1, the authors present a schematic diagram that illustrates different query-document matching paradigms in neural information retrieval (IR). The diagram compares existing approaches with a proposed late interaction paradigm.\n",
      "\n",
      "The sub-figures (a), (b), and (c) represent existing approaches, while sub-figure (d) represents the proposed late interaction paradigm. These paradigms are used to match queries with documents in neural IR systems.\n",
      "\n",
      "The authors highlight that BERT, a popular neural model, significantly improves search precision, as measured by Mean Reciprocal Rank (MRR) at the top 10 results. It shows an improvement of almost 7% compared to previous methods. However, using BERT also increases latency, with response times potentially reaching tens of thousands of milliseconds, even with high-end GPUs.\n",
      "\n",
      "This tradeoff between search precision and latency is challenging because even a small increase in query response times can impact user experience and revenue. To address this problem, recent work has explored using Natural Language Understanding (NLU) techniques to enhance traditional retrieval models like BM25.\n",
      "\n",
      "Overall, the schematic diagrams in paper_1 provide a visual representation of different query-document matching paradigms in neural IR and highlight the tradeoff between search precision and latency.\n",
      "\n",
      "\n",
      "Sources:\n",
      "parsed_file\\Paper_1.txt\n",
      "parsed_file\\Paper_2.txt\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain about the 'Schematic diagrams illustrating query–document matching paradigms in neural IR' in paper_1?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.6.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (5.3.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Requirement already satisfied: importlib-metadata<7,>=1.4 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (1.24.3)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (23.0)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<10,>=7.1.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (9.4.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (4.23.4)\n",
      "Requirement already satisfied: pyarrow>=6.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (11.0.0)\n",
      "Collecting pympler<2,>=0.9 (from streamlit)\n",
      "  Using cached Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.18 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (2.29.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.4.2-py3-none-any.whl (239 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.1.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (4.5.0)\n",
      "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
      "  Using cached tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Using cached validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "Collecting pydeck<1,>=0.8 (from streamlit)\n",
      "  Using cached pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (6.2)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\dc\\anaconda\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dc\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.17.3)\n",
      "Requirement already satisfied: toolz in c:\\users\\dc\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dc\\anaconda\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\dc\\anaconda\\lib\\site-packages (from importlib-metadata<7,>=1.4->streamlit) (3.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dc\\anaconda\\lib\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dc\\anaconda\\lib\\site-packages (from requests<3,>=2.18->streamlit) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dc\\anaconda\\lib\\site-packages (from requests<3,>=2.18->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from requests<3,>=2.18->streamlit) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dc\\anaconda\\lib\\site-packages (from requests<3,>=2.18->streamlit) (2023.5.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
      "  Using cached pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata (from tzlocal<5,>=1.1->streamlit)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from validators<1,>=0.2->streamlit) (5.1.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\dc\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dc\\anaconda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Building wheels for collected packages: validators\n",
      "  Building wheel for validators (setup.py): started\n",
      "  Building wheel for validators (setup.py): finished with status 'done'\n",
      "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19591 sha256=7b70b1a8ca0d9a1e318647fb2a3ff199c969a37fe3522b0914ed74a176417200\n",
      "  Stored in directory: c:\\users\\dc\\appdata\\local\\pip\\cache\\wheels\\82\\35\\dc\\f88ec71edf2a5596bd72a8fa1b697277e0fcd3cde83048b8bf\n",
      "Successfully built validators\n",
      "Installing collected packages: validators, tzdata, smmap, pympler, blinker, rich, pytz-deprecation-shim, pydeck, gitdb, tzlocal, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.0.1 blinker-1.6.2 gitdb-4.0.10 gitpython-3.1.32 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 rich-13.4.2 smmap-5.0.0 streamlit-1.25.0 tzdata-2023.3 tzlocal-4.3.1 validators-0.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
