A Few Brief Notes on DeepImpact, COIL, and a Conceptual
Framework for Information Retrieval Techniques
Jimmy Lin andXueguang Ma
David R. Cheriton School of Computer Science
University of Waterloo
Abstract
Recent developments in representational learn-
ing for information retrieval can be organized
in a conceptual framework that establishes two
pairs of contrasts: sparse vs. dense representa-
tions and unsupervised vs. learned representa-
tions. Sparse learned representations can fur-
ther be decomposed into expansion and term
weighting components. This framework al-
lows us to understand the relationship between
recently proposed techniques such as DPR,
ANCE, DeepCT, DeepImpact, and COIL, and
furthermore, gaps revealed by our analysis
point to “low hanging fruit” in terms of tech-
niques that have yet to be explored. We present
a novel technique dubbed “uniCOIL”, a simple
extension of COIL that achieves to our knowl-
edge the current state-of-the-art in sparse re-
trieval on the popular MS MARCO passage
ranking dataset. Our implementation using
the Anserini IR toolkit is built on the Lucene
search library and thus fully compatible with
standard inverted indexes.
1 Introduction
We present a novel conceptual framework for un-
derstanding recent developments in information re-
trieval that organizes techniques along two dimen-
sions. The ﬁrst dimension establishes the contrast
between sparse and dense vector representations
for queries and documents.1The second dimen-
sion establishes the contrast between unsupervised
and learned (supervised) representations. Figure 1
illustrates our framework.
Recent proposals for dense retrieval, exempliﬁed
by DPR (Karpukhin et al., 2020) and ANCE (Xiong
et al., 2021), but also encompassing many other
techniques (Gao et al., 2021b; Hofstätter et al.,
2020; Qu et al., 2021; Hofstätter et al., 2021; Lin
1Consistent with parlance in information retrieval, we use
“document” throughout this paper in a generic sense to refer to
the unit of retrieved text. To be more precise, our experiments
are in fact focused on passage retrieval.Dense Sparse
Supervised DPR, ANCE DeepImpact, COIL
Unsupervised LSI, LDA BM25, tf–idf
Table 1: Our conceptual framework for organizing re-
cent developments in information retrieval.
et al., 2021), can be understood as learned dense
representations for retrieval. This is formulated
as a representational learning problem where the
task is to learn (transformer-based) encoders that
map queries and documents into dense ﬁxed-width
vectors (768 dimensions is typical) in which inner
products between queries and relevant documents
are maximized, based on supervision signals from
a large dataset such as the MS MARCO passage
ranking test collection (Bajaj et al., 2018). See Lin
et al. (2020) for a survey.
Dense retrieval techniques are typically com-
pared against a bag-of-words exact match ranking
model such as BM25, which in this context can be
understood as unsupervised sparse retrieval. Al-
though it may be unnatural to describe BM25 in
this way, it is technically accurate: each document
is represented by a sparse vector where each dimen-
sion corresponds to a unique term in the vocabulary,
and the scoring function assigns a weight to each di-
mension. As with dense retrieval, query–document
scores are computed via inner products.
What about learned sparse retrieval? The most
prominent recent example of this in the literature
is DeepCT (Dai and Callan, 2019), which uses
a transformer to learn term weights based on a re-
gression model, with the supervision signal coming
from the MS MARCO passage ranking test collec-
tion.2DeepCT has an interesting “quirk”: in truth,
it only learns the term frequency (tf) component
of term weights, but still relies on the remaining
2Learning sparse representations is by no means a new idea.
The earliest example we are aware of is Wilbur (2001), who
attempted to learn global term weights using TREC data, but
the idea likely dates back even further.arXiv:2106.14807v1  [cs.IR]  28 Jun 2021parts of the BM25 scoring function via the gen-
eration of pseudo-documents. This approach also
has a weakness: it only assigns weights to terms
that are already present in the document, which
limits retrieval to exact match. This is an impor-
tant limitation that is addressed by the use of dense
representations, which are capable of capturing se-
mantic matches.
These two issues were resolved by the recently
proposed DeepImpact model (Mallia et al., 2021),
which also belongs in the family of learned sparse
representations. DeepImpact brought together two
key ideas: the use of document expansion to iden-
tify dimensions in the sparse vector that should
have non-zero weights and a term weighting model
based on a pairwise loss between relevant and non-
relevant texts with respect to a query. Expansion
terms were identiﬁed by doc2query–T5 (Nogueira
and Lin, 2019), a sequence-to-sequence model for
document expansion that predicts queries for which
a text would be relevant. Since the DeepImpact
scoring model directly predicts term weights that
are then quantized, it would be more accurate to
call these weights learned impacts, since query–
document scores are simply the sum of weights of
document terms that are found in the query. Calling
these impact scores draws an explicit connection to
a thread of research in information retrieval dating
back two decades (Anh et al., 2001).
The recently proposed COIL architecture (Gao
et al., 2021a) presents an interesting case for this
conceptual framework. Where does it belong? The
authors themselves describe COIL as “a new ex-
act lexical match retrieval architecture armed with
deep LM representations”. COIL produces repre-
sentations for each document token that are then
directly stored in the inverted index, where the
term frequency usually goes in an inverted list.
Although COIL is perhaps best described as the
intellectual descendant of ColBERT (Khattab and
Zaharia, 2020), another way to think about it within
our conceptual framework is that instead of assign-
ingscalar weights to terms in a query, the “scoring”
model assigns each term a vector “weight”. Query
evaluation in COIL involves accumulating inner
products instead of scalar weights.
Our conceptual framework highlights a ﬁnal
class of techniques: unsupervised dense represen-
tations. While there is little work in this space of
late, it does describe techniques such as LSI (Deer-
wester et al., 1990; Atreya and Elkan, 2010) andLDA (Wei and Croft, 2006), which have been previ-
ously explored. Thus, all quadrants in our proposed
conceptual framework are populated with known
examples from the literature.
2 Comments and Observations
Based on this framework, we can make a number of
interesting observations that highlight obvious next
steps in the development of retrieval techniques.
We discuss as follows:
Choice of bases. Retrieval techniques using learned
dense representations and learned sparse represen-
tations present an interesting contrast. Nearly all
recent proposals take advantage of transformers, so
that aspect of the design is not a salient difference.
The critical contrast is the basis of the vector rep-
resentations: In sparse approaches, the basis of the
vector space remains ﬁxed to the corpus vocabulary,
and thus techniques such as DeepCT, COIL, and
DeepImpact can be understood as term weighting
models. In dense approaches, the model is given
the freedom to choose a new basis derived from
transformer representations. This change in basis
allows the encoder to represent the “meaning” of
texts in relatively small ﬁxed-width vectors (com-
pared to sparse vectors that may have millions of
dimensions). This leads us to the next important
observation:
Expansions for sparse representation. Without
some form of expansion, learned sparse represen-
tations remain limited to (better) exact matching
between queries and documents. The nature of
sparse representations means that it is impractical
to consider non-zero weights for allelements in
the vector (i.e., the vocabulary space). Thus, docu-
ment expansion serves the critical role of proposing
a set of candidate terms that should receive non-
zero weights; since the number of candidate terms
is small compared to the vocabulary size, the re-
sulting vector remains sparse. Without expansion,
learned sparse representations cannot address the
vocabulary mismatch problem (Furnas et al., 1987),
because document terms not present in the query
cannot contribute any score. For DeepImpact, this
expansion is performed by doc2query–T5, but in
principle we can imagine other methods also. This
leads us to the next important observation:
Relating DeepCT, DeepImpact, and COIL. The up-
shot of the above analysis is that retrieval tech-
niques based on learned sparse representations
should be divided into an expansion model andSparse Representations MRR@10 Notes
Term Weighting Expansion
(1a) BM25 None 0.184 copied from (Nogueira and Lin, 2019)
(1b) BM25 doc2query–T5 0.277 copied from (Nogueira and Lin, 2019)
(2a) DeepCT None 0.243 copied from (Dai and Callan, 2019)
(2b) DeepCT doc2query–T5 ? no publicly reported ﬁgure
(2c) DeepImpact None ? no publicly reported ﬁgure
(2d) DeepImpact doc2query–T5 0.326 copied from (Mallia et al., 2021)
(2e) COIL-tok ( d= 32 ) None 0.341 copied from (Gao et al., 2021a)
(2f) COIL-tok ( d= 32 ) doc2query–T5 0.361 our experiment
(2g) uniCOIL None 0.315 our experiment
(2h) uniCOIL doc2query–T5 0.352 our experiment
Dense Representations MRR@10 Notes
(3a) ColBERT 0.360 copied from (Khattab and Zaharia, 2020)
(3b) ANCE 0.330 copied from (Xiong et al., 2021)
(3c) DistillBERT 0.323 copied from (Hofstätter et al., 2020)
(3d) RocketQA 0.370 copied from (Qu et al., 2021)
(3e) TAS-B 0.347 copied from (Hofstätter et al., 2021)
(3f) TCT-ColBERTv2 0.359 copied from (Lin et al., 2021)
Dense–Sparse Hybrids MRR@10 Notes
(4a) CLEAR 0.338 copied from (Gao et al., 2021b)
(4b) COIL-full 0.355 copied from (Gao et al., 2021a)
(4c) TCT-ColBERTv2 + BM25 (1a) 0.369 copied from (Lin et al., 2021)
(4d) TCT-ColBERTv2 + doc2query–T5 (1b) 0.375 copied from (Lin et al., 2021)
(4e) TCT-ColBERTv2 + DeepImpact (2d) 0.378 our experiment
(4f) TCT-ColBERTv2 + uniCOIL (2h) 0.378 our experiment
(4g) TCT-ColBERTv2 + COIL (2f) 0.382 our experiment
Table 2: Results on the development queries of the MS MARCO passage ranking task.
a term weighting model. For example, DeepCT
performs no expansion and uses a regression-based
scoring model. DeepImpact performs document ex-
pansion and uses a pairwise scoring model. COIL
performs no expansion and uses a “scoring” model
that generates a contextualized “weight vector” (in-
stead of a scalar weight). This breakdown suggests
a number of obvious experiments that help us un-
derstand the contributions of these components,
which we report next.
3 Experiments
Our proposed conceptual framework can be used
to organize results from the literature, which are
shown in Table 2 on the development queries of
the MS MARCO passage ranking task (Bajaj et al.,
2018). Some of these entries represent ﬁgures di-
rectly copied from previous papers (with references
shown), while others are novel experimental condi-
tions that we report.
The ﬁrst main block of the table shows retrieval
with sparse representations. Row (1a) shows the
BM25 baseline, and row (1b) provides the effective-
ness of doc2query–T5 expansion. In both cases, the
term weights are from the BM25 scoring function,and hence unsupervised. Learned sparse retrieval
techniques are shown in row group (2). Separat-
ing the term weighting component from the ex-
pansion component allows us to identify gaps in
model conﬁgurations that would be interesting to
explore. For example, in row (2a), DeepCT pro-
posed a regression-based term weighting model,
but performed no expansion. However, the term
weighting model can be applied to expanded doc-
uments, as in row (2b); to our knowledge, this
conﬁguration has not been publicly reported.
Similarly, DeepImpact combined doc2query–T5
as an expansion model and a term weighting model
trained with pairwise loss. To better understand
the contributions of each component, we could
run the term weighting model without document
expansion, as outlined in row (2c). This ablation
experiment was not reported in Mallia et al. (2021),
but would be interesting to conduct.
In row (2e) we report the published results of
COIL-tok (token dimension d= 32 ), which is the
sparse component in the full COIL model (which
is a dense–sparse hybrid). Through the lens of
our conceptual framework, a number of extensions
become immediately obvious. COIL can be com-bined with doc2query–T5. Using source code pro-
vided by the authors,3we trained such a model
from scratch, using the same hyperparameters as
the authors. This variant leads to a nearly two-point
gain in effectiveness, as shown in row (2f).
In another interesting extension, if we reduce the
token dimension of COIL to one, the model degen-
erates into producing scalar weights, which then
becomes directly comparable to DeepCT, row (2a)
and the “no-expansion” variant of DeepImpact, row
(2c). These comparisons isolate the effects of differ-
ent term weighting models. We dub this variant of
COIL “uniCOIL”, on top of which we can also add
doc2query–T5, which produces a fair comparison
to DeepImpact, row (2d). The original formulation
of COIL, even with a token dimension of one, is
not directly amenable to retrieval using inverted
indexes because weights can be negative. To ad-
dress this issue, we added a ReLU operation on
the output term weights of the base COIL model to
force the model to generate non-negative weights.
Once again, we retrained the model from scratch
using the same hyperparameters provided by the
authors. When encoding the corpus, we quantized
these weights into 8 bits to obtain impact scores;
query weights are similarly quantized. After these
modiﬁcations, uniCOIL is directly compatible with
inverted indexes. Our experimental results are re-
ported with the Anserini toolkit (Yang et al., 2017,
2018), which is built on Lucene.
It is no surprise that uniCOIL without doc2query–
T5, row (2g), is less effective than COIL-tok ( d=
32), row (2e). However, uniCOIL with doc2query–
T5, row (2h), outperforms COIL-tok without need-
ing any specialized retrieval infrastructure—the
weights are just impact scores, like in DeepImpact.
These results suggest that contextualized “weight
vectors” in COIL aren’t necessary to achieve good
effectiveness—adding expansion appears sufﬁcient
to make up for the lost expressivity of weight vec-
tors, as shown in row (2h) vs. row (2e). To our
knowledge, our uniCOIL model, row (2h), repre-
sents the state of the art in sparse retrieval using
learned impact weights, beating DeepImpact by
around two points.
The second main block of Table 2 provides a
number of comparable dense retrieval results from
the literature. The highest score that we are aware
of is RocketQA (Qu et al., 2021), whose effective-
ness beats all known sparse conﬁgurations. Note
3https://github.com/luyug/COILthat ColBERT (Khattab and Zaharia, 2020) uses
the more expressive MaxSim operator to compare
query and document representations; all other tech-
niques use inner products.
The ﬁnal block of Table 2 presents the results of
dense–sparse hybrids. Lin et al. (2021) reported
the results of dense–sparse hybrids when TCT-
ColBERTv2, row (3f), is combined with BM25,
row (1a), and doc2query–T5, row (1b). To this,
we added fusion with DeepImpact, uniCOIL, and
COIL-tok (d= 32 ). For a fair comparison, we fol-
lowed the same technique for combining dense and
sparse results as Lin et al. (2021), which is from Ma
et al. (2021). For each query q, we used the corre-
sponding dense and sparse techniques to retrieve
top-1k documents. The ﬁnal fusion score of each
document is calculated by sdense +ssparse . Since
the range of the two different scores are quite differ-
ent, we ﬁrst normalized the scores into range(0, 1).
Thewas tuned in the range(0, 2) with a simple
line search on a subset of the MS MARCO passage
training set.
With these hybrid combinations, we are able
to achieve, to our knowledge, the highest reported
scores on the MS MARCO passage ranking task for
single-stage techniques (i.e., no reranking). Note
that, as before, uniCOIL is compatible with stan-
dard inverted indexes, unlike COIL-tok, which re-
quires custom infrastructure.
4 Next Steps
In most recent work, dense retrieval techniques are
compared to BM25 and experiments show that they
handily win. However, this is not a fair compari-
son, since BM25 is unsupervised, whereas dense
retrieval techniques exploit supervised relevance
signals from large datasets. A more appropriate
comparison would be between learned dense vs.
sparse representations—and there, no clear win-
ner emerges at present. However, it seems clear
that they are complementary, as hybrid approaches
appear to be more effective than either alone.
An important point to make here is that neu-
ral networks, particularly transformers, have not
made sparse representations obsolete. Both dense
and sparse learned representations clearly exploit
transformers—the trick is that the latter class of
techniques then “projects” the learned knowledge
back into the sparse vocabulary space. This al-
lows us to reuse decades of innovation in inverted
indexes (e.g., integer coding techniques to com-press inverted lists) and efﬁcient query evaluation
algorithms (e.g., smart skipping to reduce query
latency): for example, the Lucene index used in
our uniCOIL experiments is only 1.3 GB, com-
pared to 40 GB for COIL-tok, 26 GB for TCT-
ColBERTv2, and 154 GB for ColBERT. We note,
however, that with dense retrieval techniques, ﬁxed-
width vectors can be approximated with binary
hash codes, yielding far more compact representa-
tions with sacriﬁcing much effectiveness (Yamada
et al., 2021). Once again, no clear winner emerges
at present.
The complete design space of modern informa-
tion retrieval techniques requires proper accounting
of the tradeoffs between output quality (effective-
ness), time (query latency), and space (index size).
Here, we have only focused on the ﬁrst aspect.
Learned representations for information retrieval
are clearly the future, but the advantages and dis-
advantages of dense vs. sparse approaches along
these dimensions are not yet fully understood. It’ll
be exciting to see what comes next!
5 Acknowledgments
This research was supported in part by the Canada
First Research Excellence Fund and the Natural Sci-
ences and Engineering Research Council (NSERC)
of Canada. Computational resources were provided
by Compute Ontario and Compute Canada.
References
V o Ngoc Anh, Owen de Kretser, and Alistair Moffat.
2001. Vector-space ranking with effective early ter-
mination. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2001) ,
pages 35–42, New Orleans, Louisiana.
Avinash Atreya and Charles Elkan. 2010. Latent se-
mantic indexing (LSI) fails for TREC collections.
SIGKDD Explorations , 12(2):5–10.
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng,
Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Ti-
wary, and Tong Wang. 2018. MS MARCO: A Hu-
man Generated MAchine Reading COmprehension
Dataset. arXiv:1611.09268v3 .
Zhuyun Dai and Jamie Callan. 2019. Context-aware
sentence/passage term importance estimation for
ﬁrst stage retrieval. arXiv:1910.10687 .
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.Indexing by latent semantic analysis. Journal of
the Association for Information Science , 41(6):391–
407.
George W. Furnas, Thomas K. Landauer, Louis M.
Gomez, and Susan T. Dumais. 1987. The vo-
cabulary problem in human-system communication.
Communications of the ACM , 30(11):964–971.
Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021a.
COIL: Revisit exact lexical match in information
retrieval with contextualized inverted list. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
3030–3042.
Luyu Gao, Zhuyun Dai, Tongfei Chen, Zhen Fan, Ben-
jamin Van Durme, and Jamie Callan. 2021b. Com-
plementing lexical retrieval with semantic residual
embedding. In Proceedings of the 43rd European
Conference on Information Retrieval (ECIR 2021),
Part I , pages 146–160.
Sebastian Hofstätter, Sophia Althammer, Michael
Schröder, Mete Sertkan, and Allan Hanbury.
2020. Improving efﬁcient neural ranking mod-
els with cross-architecture knowledge distillation.
arXiv:2010.02666 .
Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong
Yang, Jimmy Lin, and Allan Hanbury. 2021. Ef-
ﬁciently teaching an effective dense retriever with
balanced topic aware sampling. In Proceedings of
the 44th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval (SIGIR 2021) .
Vladimir Karpukhin, Barlas O ˘guz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 6769–
6781.
Omar Khattab and Matei Zaharia. 2020. ColBERT: Ef-
ﬁcient and effective passage search via contextual-
ized late interaction over BERT. In Proceedings of
the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR 2020) , pages 39–48.
Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.
2020. Pretrained transformers for text ranking:
BERT and beyond. arXiv:2010.06467 .
Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.
2021. In-batch negatives for knowledge distillation
with tightly-coupled teachers for dense retrieval. In
Proceedings of the 6th Workshop on Representation
Learning for NLP .
Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy
Lin. 2021. A replication study of dense passage re-
triever. arXiv:2104.05740 .Antonio Mallia, Omar Khattab, Torsten Suel, and
Nicola Tonellotto. 2021. Learning passage impacts
for inverted indexes. In Proceedings of the 44th An-
nual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval
(SIGIR 2021) .
Rodrigo Nogueira and Jimmy Lin. 2019. From
doc2query to docTTTTTquery.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang
Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu,
and Haifeng Wang. 2021. RocketQA: An opti-
mized training approach to dense passage retrieval
for open-domain question answering. In Proceed-
ings of the 2021 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
5835–5847.
Xing Wei and W. Bruce Croft. 2006. LDA-based doc-
ument models for ad-hoc retrieval. In Proceedings
of the 29th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval (SIGIR 2006) , pages 178–185, Seattle,
Washington.
W. John Wilbur. 2001. Global term weights for docu-
ment retrieval learned from TREC data. Journal of
Information Science , 27(5):303–310.
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
Jialin Liu, Paul N. Bennett, Junaid Ahmed, and
Arnold Overwijk. 2021. Approximate nearest neigh-
bor negative contrastive learning for dense text re-
trieval. In Proceedings of the 9th International Con-
ference on Learning Representations (ICLR 2021) .
Ikuya Yamada, Akari Asai, and Hannaneh Ha-
jishirzi. 2021. Efﬁcient passage retrieval with
hashing for open-domain question answering.
arXiv:2106.00882 .
Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini:
enabling the use of Lucene for information retrieval
research. In Proceedings of the 40th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR 2017) ,
pages 1253–1256, Tokyo, Japan.
Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini:
reproducible ranking baselines using Lucene. Jour-
nal of Data and Information Quality , 10(4):Article
16.